Thinking1:
>>我认为用户标签有两个主要作用：1）帮助业务更好的认识自己面对的是什么样的用户，提升认知；2）完成业务发起的针对用户的推荐（内容、商品或者活动），完成促活
、召回等。

Thinking2:
>>1)基础用户标签体系：性别（男女）、年龄段、地域、职业2)采用无监督的聚类算法，查看不同类别用户的特征，从中抽取标签体系。

Thinking3:
accuracy（准确率）= （TP+TN）/（TP+TN+FP+FN）
precision（精确率）= TP/(TP+FP)
>> 用一个例子来解释两者的不同，比如有一个图像分类器，功能是选出输入的图片哪些是猫咪。precision(精确率)代表了这个图像分类器正确识别出猫的能力，例如输入
的照片中有10张照片是猫，这个分类器识别出了8张，则precision=0.8，这个指标并不考虑分类器识别不是猫的能力，换句话说，分类器为了提高prcision而把一些不是
猫的照片识别为猫了。accuracy(准确率)则将分辨是猫和不是猫的能力总和在一起了，但这个指标的问题是不能单独反映识别是猫和识别不是猫的能力，会出现TP不高但是
TN高的情况，在这个分类器功能设定的前提下，就很尴尬了。

Thinking4:
1)基础标签体系：餐厅地理位置（国家、省市、商圈）、餐厅菜系、口味、就餐环境、价格、适合人群（比如适合情侣约会等）
2）用无监督算法挖掘潜在标签

Thinking5:
首先，mnist数据集是个同质数据，不是异质数据（即每个维度有独立的含义，比如年龄、性别、学历等）。这个训练集算大还是小，我不会评估。通常情况下，如果是小训
练集，高偏差/低方差的分类器（例如，朴素贝叶斯NB）要比低偏差/高方差大分类的优势大（例如，KNN），因为后者会发生过拟合（overfiting）。然而，随着你训练集
的增长，模型对于原数据的预测能力就越好，偏差就会降低，此时低偏差/高方差的分类器就会渐渐的表现其优势（因为它们有较低的渐近误差），而高偏差分类器这时已经不
足以提供准确的模型了。
以下是各个算法的优缺点：
1.Logistic Regression
* 要求数据集符合高斯分布
* 不能很好的处理大量多类特征或变量，高维空间可能不如SVM（我也不确定这个点）
* 有正则化方法可选，不用像担心NB那样担心协变量。
* 但容易出现欠拟合
* 很适合做baseline
2.CART ID3
* 对数据集分布没有假设要求
* 不支持在线学习
* 容易出现过拟合
* 对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只要是使用了信息增益，都有这个缺点，如RF）
3.LDA
* 要求数据集符合高斯分布
4.朴素贝叶斯
* 要求特征值独立分布
* 对缺失数据不太敏感
5.SVM
* 可以很好地应用于高维数据，避免了维度灾难问题。而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行的很好。不过，核函数自己做的话很难。。。 
* 对缺失数据敏感
6.KNN
* 对数据集无假设要求，但必须做归一化处理，不然影响距离权重
* 计算量相对比较大
* 如果数据集存在样本不均衡问题，表现会比较差
7.Adaboost 
8.XGBoost 
9.TPOT 
10.keras
*7-10无明显优缺点。
>>其他可用的还有：LQD,Naive Bayes,Ensemble methods

# 使用SimpleTagBased算法对Delicious2K数据进行推荐
# 原始数据集：https://grouplens.org/datasets/hetrec-2011/
# 数据格式：userID     bookmarkID     tagID     timestamp
import random
import math
import operator
import pandas as pd

file_path = "./user_taggedbookmarks-timestamps.dat"
# 字典类型，保存了user对item的tag，即{userid: {item1:[tag1, tag2], ...}}
records = {}
# 训练集，测试集
train_data = dict()
test_data = dict()
# 用户标签，商品标签
user_tags = dict()
tag_items = dict()
user_items = dict()

# 使用测试集，计算准确率和召回率
def precisionAndRecall(N):
    hit = 0
    h_recall = 0
    h_precision = 0
    for user,items in test_data.items():
        if user not in train_data:
            continue
        # 获取Top-N推荐列表
        rank = recommend(user, N)
        for item,rui in rank:
            if item in items:
                hit = hit + 1
        h_recall = h_recall + len(items)
        h_precision = h_precision + N
    #print('一共命中 %d 个, 一共推荐 %d 个, 用户设置tag总数 %d 个' %(hit, h_precision, h_recall))
    # 返回准确率 和 召回率
    return (hit/(h_precision*1.0)), (hit/(h_recall*1.0))

# 对用户user推荐Top-N
def recommend(user, N):
    recommend_items=dict()
    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和
    tagged_items = user_items[user]     
    for tag, wut in user_tags[user].items():
        #print(self.user_tags[user].items())
        for item, wti in tag_items[tag].items():
            if item in tagged_items:
                continue
            #print('wut = %s, wti = %s' %(wut, wti))
            if item not in recommend_items:
                recommend_items[item] = wut * wti
            else:
                recommend_items[item] = recommend_items[item] + wut * wti
    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]

# 使用测试集，对推荐结果进行评估
def testRecommend():
    print("推荐结果评估")
    print("%3s %10s %10s" % ('N',"精确率",'召回率'))
    for n in [5,10,20,40,60,80,100]:
        precision,recall = precisionAndRecall(n)
        print("%3d %10.3f%% %10.3f%%" % (n, precision * 100, recall * 100))

# 数据加载
def load_data():
    print("开始数据加载...")
    df = pd.read_csv(file_path, sep='\t')
    for i in range(len(df)):
        uid = df['userID'][i]
        iid = df['bookmarkID'][i]
        tag = df['tagID'][i]
        # 键不存在时，设置默认值{}
        records.setdefault(uid,{})
        records[uid].setdefault(iid,[])
        records[uid][iid].append(tag)
    print("数据集大小为 %d." % (len(df)))
    print("设置tag的人数 %d." % (len(records)))
    print("数据加载完成\n")

# 将数据集拆分为训练集和测试集
def train_test_split(ratio, seed=100):
    random.seed(seed)
    for u in records.keys():
        for i in records[u].keys():
            # ratio比例设置为测试集
            if random.random()<ratio:
                test_data.setdefault(u,{})
                test_data[u].setdefault(i,[])
                for t in records[u][i]:
                    test_data[u][i].append(t)
            else:
                train_data.setdefault(u,{})
                train_data[u].setdefault(i,[])
                for t in records[u][i]:
                    train_data[u][i].append(t)        
    print("训练集样本数 %d, 测试集样本数 %d" % (len(train_data),len(test_data)))

# 设置矩阵 mat[index, item] = 1
def addValueToMat(mat, index, item, value=1):
    if index not in mat:
        mat.setdefault(index,{})
        mat[index].setdefault(item,value)
    else:
        if item not in mat[index]:
            mat[index][item] = value
        else:
            mat[index][item] += value

# 使用训练集，初始化user_tags, tag_items, user_items
def initStat():
    records=train_data
    for u,items in records.items():
        for i,tags in items.items():
            for tag in tags:
                #print tag
                # 用户和tag的关系
                addValueToMat(user_tags, u, tag, 1)
                # tag和item的关系
                addValueToMat(tag_items, tag, i, 1)
                # 用户和item的关系
                addValueToMat(user_items, u, i, 1)
    print("user_tags, tag_items, user_items初始化完成.")
    print("user_tags大小 %d, tag_items大小 %d, user_items大小 %d" % (len(user_tags), len(tag_items), len(user_items)))

# 数据加载
load_data()
# 训练集，测试集拆分，20%测试集
train_test_split(0.2)
initStat()
testRecommend()

### 使用NormTagBased算法

# 对用户user推荐Top-N
def recommend_normtagbased(user, N):
    recommend_items=dict()
    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut/这个用户一共打了多少标签 ut_counts), 乘以 商品被打上相同标签的次数 wti/这个tag出现的总次数t_counts）之和
    # 计算这个用户一共打了多少标签 ut_counts
    ut_counts = sum(user_tags[user].values())
    tagged_items = user_items[user]     
    for tag, wut in user_tags[user].items():
        # 计算这个tag出现的总次数t_counts
        t_counts = sum(tag_items[tag].values())
        #print(self.user_tags[user].items())
        for item, wti in tag_items[tag].items():
            if item in tagged_items:
                continue
            #print('wut = %s, wti = %s' %(wut, wti))
            if item not in recommend_items:
                recommend_items[item] = (wut/ut_counts) * (wti/t_counts)
            else:
                recommend_items[item] = recommend_items[item] + (wut/ut_counts) * (wti/t_counts)
    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]

# 使用测试集，计算准确率和召回率
def precisionAndRecall_normtagbased(N):
    hit = 0
    h_recall = 0
    h_precision = 0
    for user,items in test_data.items():
        if user not in train_data:
            continue
        # 获取Top-N推荐列表
        rank = recommend_normtagbased(user, N)
        for item,rui in rank:
            if item in items:
                hit = hit + 1
        h_recall = h_recall + len(items)
        h_precision = h_precision + N
    #print('一共命中 %d 个, 一共推荐 %d 个, 用户设置tag总数 %d 个' %(hit, h_precision, h_recall))
    # 返回准确率 和 召回率
    return (hit/(h_precision*1.0)), (hit/(h_recall*1.0))

# 使用测试集，对推荐结果进行评估
def testRecommend_normtagbased():
    print("推荐结果评估")
    print("%3s %10s %10s" % ('N',"精确率",'召回率'))
    for n in [5,10,20,40,60,80,100]:
        precision,recall = precisionAndRecall_normtagbased(n)
        print("%3d %10.3f%% %10.3f%%" % (n, precision * 100, recall * 100))

# 数据加载
load_data()
# 训练集，测试集拆分，20%测试集
train_test_split(0.2)
initStat()
testRecommend_normtagbased()

### 使用TagBased-TFIDF算法

# 该标签被多少个不同的用户使用过
tag_user = dict()
records=train_data
for u,items in records.items():
    for i,tags in items.items():
        for tag in tags:
            addValueToMat(tag_user, tag, u ,1)

# 对用户user推荐Top-N
def recommend_tagbasedTFIDF(user, N):
    recommend_items=dict()
    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut/该tag被多少不同的用户使用过 tu_counts), 乘以 商品被打上相同标签的次数 wti）之和
    tagged_items = user_items[user]     
    for tag, wut in user_tags[user].items():
        #该tag被多少不同的用户使用过 tu_counts
        tu_counts = sum(tag_user[tag].values())
        #print(self.user_tags[user].items())
        for item, wti in tag_items[tag].items():
            if item in tagged_items:
                continue
            #print('wut = %s, wti = %s' %(wut, wti))
            if item not in recommend_items:
                recommend_items[item] = (wut/tu_counts) * wti
            else:
                recommend_items[item] = recommend_items[item] + (wut/tu_counts) * wti
    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]

# 使用测试集，计算准确率和召回率
def precisionAndRecall_tagbasedTFIDF(N):
    hit = 0
    h_recall = 0
    h_precision = 0
    for user,items in test_data.items():
        if user not in train_data:
            continue
        # 获取Top-N推荐列表
        rank = recommend_tagbasedTFIDF(user, N)
        for item,rui in rank:
            if item in items:
                hit = hit + 1
        h_recall = h_recall + len(items)
        h_precision = h_precision + N
    #print('一共命中 %d 个, 一共推荐 %d 个, 用户设置tag总数 %d 个' %(hit, h_precision, h_recall))
    # 返回准确率 和 召回率
    return (hit/(h_precision*1.0)), (hit/(h_recall*1.0))

# 使用测试集，对推荐结果进行评估
def testRecommend_tagbasedTFIDF():
    print("推荐结果评估")
    print("%3s %10s %10s" % ('N',"精确率",'召回率'))
    for n in [5,10,20,40,60,80,100]:
        precision,recall = precisionAndRecall_tagbasedTFIDF(n)
        print("%3d %10.3f%% %10.3f%%" % (n, precision * 100, recall * 100))

# 数据加载
load_data()
# 训练集，测试集拆分，20%测试集
train_test_split(0.2)
initStat()
testRecommend_tagbasedTFIDF()
