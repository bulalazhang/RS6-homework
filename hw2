import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as stats
from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import ExtraTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_validate
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import RidgeClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
import itertools
from scipy.stats.stats import pearsonr
pd.options.display.max_columns = None
pd.options.display.max_rows = None

#读取文件
file1 = 'train.csv'
file2 = 'test.csv'
train = pd.read_csv(file1)
test = pd.read_csv(file2)

train.head(5)

# feature processing

#查看变量
train.columns

#提取特征,'Attrition','Over18','user_id','EmployeeCount'排除
target = 'Attrition'
Y_train = train[target] 
Y_train = np.where(Y_train=='Yes',1,0)
features = [x for x in train.columns if x not in ['Attrition','Over18','user_id','EmployeeCount']]
train_ex = train[features]
test_ex = test[features]
feature_ex = pd.concat([train_ex,test_ex])

#变量分类
continuous = ['DailyRate','HourlyRate','MonthlyIncome','MonthlyRate','PercentSalaryHike']
discrete = ['Age','NumCompaniesWorked','StandardHours','TotalWorkingYears','TrainingTimesLastYear','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']
nominal = ['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','OverTime']
ordinal = ['DistanceFromHome','Education','EmployeeNumber','EnvironmentSatisfaction','JobLevel','JobInvolvement','JobSatisfaction','PerformanceRating','RelationshipSatisfaction','StockOptionLevel','WorkLifeBalance']
numerical = continuous + discrete


#创造新的特征
# 小数点特征，主要是百分比
flowCols = ['PercentSalaryHike']
# 连续数特征，都是薪水特征
salaryCols = ['DailyRate','HourlyRate','MonthlyIncome','MonthlyRate']
# 离散特征,主要是小时数和年限
vsCols =  ['Age','NumCompaniesWorked','StandardHours','TotalWorkingYears','TrainingTimesLastYear','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']
# 算出不同离散特征下面的薪水金额。 
for i in salaryCols:
    for j in vsCols:
        feature_ex["{}/{}".format(i, j)] = feature_ex[i].values/(feature_ex[j].values+1)
# 离散特征， 主要是抓占比。
for i in vsCols:
    for j in vsCols:
        if i==j:
            continue
        feature_ex["{}×{}".format(i, j)] = feature_ex[i].values*(feature_ex[j].values+1)
        feature_ex["{}/{}".format(i, j)] = feature_ex[i].values/(feature_ex[j].values+1)
        feature_ex["{}-{}".format(i, j)] = feature_ex[i].values-(feature_ex[j].values+1)
    
# 薪水特征。
for i in salaryCols:
    for j in salaryCols:
        if i==j:
            continue
        feature_ex["{}/{}".format(i, j)] = feature_ex[i].values/(feature_ex[j].values+1)
        feature_ex["{}*{}".format(i, j)] = feature_ex[i].values*(feature_ex[j].values+1)
        feature_ex["{}-{}".format(i, j)] = feature_ex[i].values-(feature_ex[j].values+1)


#用OneHotEncoder处理ordinal,
dum_label = ['BusinessTravel','Department','EducationField','JobRole']
ohe = OneHotEncoder(sparse=False)
dum = feature_ex[dum_label]
dum = ohe.fit_transform(dum)
feature_ex = feature_ex.drop(columns=dum_label)
feature_ex = feature_ex.join(pd.DataFrame(dum))

#处理二元分类变量'Gender','MaritalStatus','OverTime'
feature_ex['Gender'] = np.where(feature_ex['Gender']=='Male',1,0)
feature_ex['MaritalStatus'] = np.where(feature_ex['MaritalStatus']=='Married',1,0)
feature_ex['OverTime'] = np.where(feature_ex['OverTime']=='Yes',1,0)

#数据集
trainLen = train_ex.shape[0]
X_train = feature_ex[:trainLen]
X_test = feature_ex[trainLen:]

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

## MODEL SELECTION


#set up model selection funtion
def test_model(X,y):

    
    estimator_list = [
        SGDClassifier(),
        RidgeClassifier(),
        LogisticRegression(),
        XGBClassifier(),
        DecisionTreeClassifier(),
        ExtraTreeClassifier(),
        SVC(),
        KNeighborsClassifier(),
        GradientBoostingClassifier()    
    ]

    cv_split = ShuffleSplit(n_splits=6, train_size=0.7, test_size=0.2, random_state=168)
    df_columns = ['Name', 'Parameters', 'Train Accuracy Mean', 'Test Accuracy Mean', 'Test Accuracy Std', 'Comsumed Time']
    df = pd.DataFrame(columns=df_columns)

    row_index = 0
    for estimator in estimator_list:
        df.loc[row_index, 'Name'] = estimator.__class__.__name__
        df.loc[row_index, 'Parameters'] = str(estimator.get_params())
        cv_results = cross_validate(estimator, X,y, cv=cv_split,return_train_score=True)
        #print(cv_results)
        df.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_score'].mean()
        df.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()
        df.loc[row_index, 'Test Accuracy Std'] = cv_results['test_score'].std()
        df.loc[row_index, 'Comsumed Time'] = cv_results['fit_time'].mean()
        print(row_index, estimator.__class__.__name__)
        print(cv_results['test_score'])
        row_index += 1
    df = df.sort_values(by='Test Accuracy Mean', ascending=False)
    return df

test_model(X_train,Y_train)


#max_depth 和 min_weight参数调优
xgb1 = XGBClassifier(
    booster = 'gbtree',
    learning_rate = 0.01,
    n_estimators = 140,
    max_depth = 5, 
    min_child_weight =1,
    gamma = 0,
    subsample = 0.8,
    colsample_bytree = 0.8,
    objective = 'binary:logistic',
    nthread = 4,
    scale_pos_weight = 1,                                   
    seed = 27
)

param_test1 = {
    'max_depth':range(3,10,2),
    'min_child_weight':range(1,6,2)
}

gsearch1 = GridSearchCV(
    estimator = xgb1,
    param_grid = param_test1,
    scoring = 'roc_auc',
    n_jobs = 4,
    iid = False,
    cv=5,
    return_train_score=True
)

gsearch1.fit(X_train,Y_train)
results = gsearch1.cv_results_
best_estimator = gsearch1.best_estimator_


#gamma参数调优
param_test2 = {
    'gamma':[i/10.0 for i in range(0,8)]
}

gsearch2 = GridSearchCV(
    estimator = best_estimator,
    param_grid = param_test2,
    scoring = 'roc_auc',
    n_jobs = 4,
    iid = False,
    cv=5,
    return_train_score=True
)

gsearch2.fit(X_train,Y_train)
results2 = gsearch2.cv_results_
best_estimator = gsearch2.best_estimator_

#调整subsample和colsample_bytree参数
#取0.6，0.7，0.8，0.9作为起始值
param_test3 ={
    'subsample':[i/10.0 for i in range(6,10)],
    'colsample_bytree':[i/10.0 for i in range(6,10)]
}

gsearch3 = GridSearchCV(
    estimator = best_estimator,
    param_grid = param_test3,
    scoring = 'roc_auc',
    n_jobs = 4,
    iid = False,
    cv=5,
    return_train_score=True
)

gsearch3.fit(X_train,Y_train)
results3 = gsearch3.cv_results_
best_estimator = gsearch3.best_estimator_

#调整正则化参数
param_test4 = {
    'reg_alpha':[1e-5,1e-2,0.1,1,100]
}
gsearch4 = GridSearchCV(
    estimator = best_estimator,
    param_grid = param_test4,
    scoring = 'roc_auc',
    n_jobs = 4,
    iid = False,
    cv=5,
    return_train_score=True
)

gsearch4.fit(X_train,Y_train)
results4 = gsearch4.cv_results_
gsearch4.best_estimator_,gsearch4.best_params_,gsearch4.best_score_

y_predict = gsearch4.predict(X_test)

y_predict

test_id = test['user_id']

submission = pd.DataFrame({"user_id":test_id, "Attrition": y_predict})
submission.to_csv('submission.csv',index=False)
