Thinking1:
>>我认为用户标签有两个主要作用：1）帮助业务更好的认识自己面对的是什么样的用户，提升认知；2）完成业务发起的针对用户的推荐（内容、商品或者活动），完成促活
、召回等。

Thinking2:
>>1)基础用户标签体系：性别（男女）、年龄段、地域、职业2)采用无监督的聚类算法，查看不同类别用户的特征，从中抽取标签体系。

Thinking3:
accuracy（准确率）= （TP+TN）/（TP+TN+FP+FN）
precision（精确率）= TP/(TP+FP)
>> 用一个例子来解释两者的不同，比如有一个图像分类器，功能是选出输入的图片哪些是猫咪。precision(精确率)代表了这个图像分类器正确识别出猫的能力，例如输入
的照片中有10张照片是猫，这个分类器识别出了8张，则precision=0.8，这个指标并不考虑分类器识别不是猫的能力，换句话说，分类器为了提高prcision而把一些不是
猫的照片识别为猫了。accuracy(准确率)则将分辨是猫和不是猫的能力总和在一起了，但这个指标的问题是不能单独反映识别是猫和识别不是猫的能力，会出现TP不高但是
TN高的情况，在这个分类器功能设定的前提下，就很尴尬了。

Thinking4:
1)基础标签体系：餐厅地理位置（国家、省市、商圈）、餐厅菜系、口味、就餐环境、价格、适合人群（比如适合情侣约会等）
2）用无监督算法挖掘潜在标签

Thinking5:
首先，mnist数据集是个同质数据，不是异质数据（即每个维度有独立的含义，比如年龄、性别、学历等）。这个训练集算大还是小，我不会评估。通常情况下，如果是小训
练集，高偏差/低方差的分类器（例如，朴素贝叶斯NB）要比低偏差/高方差大分类的优势大（例如，KNN），因为后者会发生过拟合（overfiting）。然而，随着你训练集
的增长，模型对于原数据的预测能力就越好，偏差就会降低，此时低偏差/高方差的分类器就会渐渐的表现其优势（因为它们有较低的渐近误差），而高偏差分类器这时已经不
足以提供准确的模型了。
以下是各个算法的优缺点：
1.Logistic Regression
* 要求数据集符合高斯分布
* 不能很好的处理大量多类特征或变量，高维空间可能不如SVM（我也不确定这个点）
* 有正则化方法可选，不用像担心NB那样担心协变量。
* 但容易出现欠拟合
* 很适合做baseline
2.CART ID3
* 对数据集分布没有假设要求
* 不支持在线学习
* 容易出现过拟合
* 对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只要是使用了信息增益，都有这个缺点，如RF）
3.LDA
* 要求数据集符合高斯分布
4.朴素贝叶斯
* 要求特征值独立分布
* 对缺失数据不太敏感
5.SVM
* 可以很好地应用于高维数据，避免了维度灾难问题。而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行的很好。不过，核函数自己做的话很难。。。 
* 对缺失数据敏感
6.KNN
* 对数据集无假设要求，但必须做归一化处理，不然影响距离权重
* 计算量相对比较大
* 如果数据集存在样本不均衡问题，表现会比较差
7.Adaboost 
8.XGBoost 
9.TPOT 
10.keras
*7-10无明显优缺点。
>>其他可用的还有：LQD,Naive Bayes,Ensemble methods,
